{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic PyTorch tutorial\n",
    "\n",
    "This tutorial will serve as a simple introduction to PyTorch. It will cover the following: \n",
    "1. Declare and initialize a neural network (Multi-layer perceptron)\n",
    "2. Load train/test data (MNIST)\n",
    "3. Train a neural network to perform the task\n",
    "4. Evaluate test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTING A BUNCH OF PYTORCH LIBRARIES \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns; sns.set()\n",
    "import copy, random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading train/test data (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset =   torchvision.datasets.MNIST('MNIST/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST('MNIST/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader( testset, batch_size=4, shuffle=True)\n",
    "\n",
    "classes = ('0','1', '2', '3', '4','5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaring a neural network (Multi-layer perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP DEFINITION\n",
    "\n",
    "### This network has 4 layers: \n",
    "### Input [784 x 1] --> Hidden-1 [300 x 1] --> Hidden-2 [100 x 1] --> Output [10 x 1]\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP_MNIST(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP_MNIST, self).__init__()\n",
    "        random_seed = 1;\n",
    "        torch.manual_seed(random_seed)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(784,300),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(300,100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100,10)\n",
    "            )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.fc(x);\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the weights and biases of the neural network. This is a Xavier initialization; \n",
    "# Optimal initialization of NN's is an entire field in itself :D \n",
    "\n",
    "def initialize_network(net):\n",
    "\n",
    "    random_seed = 1;\n",
    "    torch.manual_seed(random_seed)\n",
    "    for m in net.modules():\n",
    "        torch.manual_seed(random_seed)\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight);\n",
    "            nn.init.uniform_(m.bias);\n",
    "    \n",
    "    return net\n",
    "    '''\n",
    "    for name, param in autoenc_ch.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.data)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to evaluate the test accuracy of the network on test-data from MNIST\n",
    "\n",
    "def generalization_acc(net_mod):\n",
    "\n",
    "    correct = 0;\n",
    "    total = 0\n",
    "    batch_size = 4;\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.view(batch_size, -1)\n",
    "            outputs = net_mod(images);\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    #print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "    return (100 * correct / total);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to evaluate the train-accuracy of the network on the train-data from MNIST\n",
    "## (NOTE): There may be cases where the network has a very high train-accuracy but low test-accuracy --> commonly called overfitting\n",
    "\n",
    "def train_accuracyMNIST(net_mod):\n",
    "\n",
    "    correct = 0;\n",
    "    total = 0\n",
    "    batch_size = 4;\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "            images = images.view(batch_size, -1)\n",
    "            outputs = net_mod(images);\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 60000 train images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    return (100 * correct / total);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining a Loss function for the neural network. \n",
    "# here, we use the cross-entropy loss. One can use L2 loss (any loss that is appropriate for the task at hand)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "num_epochs = 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for training neural network \n",
    "\n",
    "def trainNetwork_MNIST(net, wtsDict, num_epochs=2):\n",
    "\n",
    "    ## net: corresponds to the neural network that requires training\n",
    "    ## wtsDict: dictionary of weights, if you want to store the weights of intermediate networks while training (not reqd for conventional purposes)\n",
    "    ## num_epochs: number of epochs networks are to be trained for. One epoch is training the network on all train-datapoints in the batch\n",
    "    \n",
    "    \n",
    "    #storeWts_manifold2(net, wtsDict)\n",
    "    \n",
    "    ## Choosing an optimizer. Here, we choose Stochastic gradient descent (SGD) with learning rate=0.001 and momentum = 0.9\n",
    "    optimizer = optim.SGD(net.parameters(),lr=0.001, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        running_loss = 0.0;\n",
    "        batch_size = 4;\n",
    "\n",
    "        ctr_run = 0;\n",
    "        for i in range(0, 60000, batch_size):\n",
    "                \n",
    "            image_input = torch.empty(batch_size,784)\n",
    "            labels = torch.empty(batch_size);\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                img, label = trainset[i+j]\n",
    "                img = img[0,:,:];\n",
    "                img = img.view(1,784)\n",
    "        \n",
    "                image_input[j,:] = img;\n",
    "                labels[j] = label;\n",
    "\n",
    "            output = net(image_input);\n",
    "            loss = criterion(output, labels.long());\n",
    "\n",
    "            # =====================backward==================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #plot_grad_flow(autoenc_ch.named_parameters)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if ctr_run % 2000 == 1999: # print every 8000 samples\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, ctr_run + 1, running_loss / 2000))\n",
    "                running_loss = 0.0;\n",
    "                #storeWts_manifold2(net, wtsDict); \n",
    "                #calcLoss_network.append(calculateLoss_network(net))\n",
    "                \n",
    "            ctr_run += 1;\n",
    "\n",
    "        acc = train_accuracyMNIST(net);\n",
    "        print(\"within func\")\n",
    "        \n",
    "        \n",
    "        ## I terminate training when accuracy reaches 98%; \n",
    "        ## One could train for a fixed number of epochs or until the network reaches a particular accuracy.\n",
    "        if num_epochs != 2:\n",
    "            # Evaluate if train accuracy is greater than 98%\n",
    "            if acc>98:\n",
    "                #storeWts_manifold2(net, wtsDict)\n",
    "                return (epoch+1, acc)\n",
    "        \n",
    "    return (epoch+1, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.589\n",
      "[1,  4000] loss: 0.291\n",
      "[1,  6000] loss: 0.208\n",
      "[1,  8000] loss: 0.181\n",
      "[1, 10000] loss: 0.157\n",
      "[1, 12000] loss: 0.147\n",
      "[1, 14000] loss: 0.129\n",
      "Accuracy of the network on the 60000 train images: 96 %\n",
      "within func\n",
      "[2,  2000] loss: 0.103\n",
      "[2,  4000] loss: 0.097\n",
      "[2,  6000] loss: 0.092\n",
      "[2,  8000] loss: 0.087\n",
      "[2, 10000] loss: 0.084\n",
      "[2, 12000] loss: 0.083\n",
      "[2, 14000] loss: 0.074\n",
      "Accuracy of the network on the 60000 train images: 97 %\n",
      "within func\n",
      "[3,  2000] loss: 0.065\n",
      "[3,  4000] loss: 0.056\n",
      "[3,  6000] loss: 0.056\n",
      "[3,  8000] loss: 0.054\n",
      "[3, 10000] loss: 0.056\n",
      "[3, 12000] loss: 0.054\n",
      "[3, 14000] loss: 0.048\n",
      "Accuracy of the network on the 60000 train images: 98 %\n",
      "within func\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d29e5d6a4b73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnet_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_MNIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mreturnTime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalcLoss_network\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainNetwork_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwtsDictionary_landscape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "## Calling all the functions [to train the net]\n",
    "\n",
    "net_MNIST = MLP_MNIST();\n",
    "\n",
    "wtsDictionary_landscape = {};\n",
    "ctr = 1;\n",
    "for n,p in net_MNIST.named_parameters():\n",
    "        \n",
    "    temp = list(p.view(1,-1).size());\n",
    "    wtsDictionary_landscape[ctr] = np.array([]).reshape(0,temp[1]);\n",
    "    ctr += 1\n",
    "    \n",
    "net_train = copy.deepcopy(net_MNIST);\n",
    "[returnTime, acc, calcLoss_network] = trainNetwork_MNIST(net_train, wtsDictionary_landscape, 5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
